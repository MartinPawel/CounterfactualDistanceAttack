{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model stuff\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_gauss = {\n",
    "    'n': 10000,                            # n_train -> 2500\n",
    "    'd': [50],                             # data dimensions\n",
    "    'sigma2': 1,                           # data variance\n",
    "    'frac_important_features': 0.1,        # fraction of relevant features\n",
    "    'sigma2_eps': 0.1,                     # label noise\n",
    "    'corr': 0.0,                           # correlation between features\n",
    "    'fit_sgd': False,                      # whether linear model should be fitted with sgd\n",
    "    'n_shadow_models': 200,                # number of shadow models to train for LRT test versions\n",
    "    'frac': 0.85,                          # fraction of samples used for resampling scheme within shadow model train pipeline\n",
    "    'epsd': 1e-45,                         # constant to ensure stable loss evaluations\n",
    "    's': 0                                 # target score in logit space\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lin_data(d:int, synth_gauss: dict):\n",
    "    \n",
    "    '''\n",
    "    This method generates data from the data generating process described above. \n",
    "    ---------------------------------------------------------------------------\n",
    "        Input: data dimension (int), snyth gauss (dictionary)\n",
    "        Output: X, Y (np.arrays); training set size (int)\n",
    "    '''\n",
    "    \n",
    "    n = synth_gauss['n']\n",
    "    n_train = int(n/4)\n",
    "    print(f'Number of training samples: {n_train}')\n",
    "\n",
    "    n_train_half = int(2*int(n/2))\n",
    "    sigma2 = synth_gauss['sigma2']\n",
    "    frac_important_features = synth_gauss['frac_important_features']\n",
    "    d_relevant = int(np.floor(d * frac_important_features))\n",
    "    mu = np.zeros(d)\n",
    "    Sigma2 = np.eye(d) * sigma2\n",
    "    Sigma2 = np.ones((d, d)) * synth_gauss['corr'] + np.eye(d) * sigma2\n",
    "\n",
    "\n",
    "    ### true parameter vector ###\n",
    "    beta0 = np.zeros(d)\n",
    "    beta = np.random.random(d) * 2 - 1                            # Coefficient randomly chosen from [-1,1]^d \n",
    "    quantile = np.quantile(np.abs(beta), 1 - frac_important_features)\n",
    "    indeces = np.where(np.abs(beta) >= quantile)[0]               # Determine indeces of relevant coefficients\n",
    "    \n",
    "    beta0[0:d_relevant] = beta[indeces]\n",
    "    beta1 = beta0 / np.linalg.norm(beta0, ord=2)                  # Normalize to have unit vector 1\n",
    "\n",
    "    ### error variance parameters & distribution of errors ###\n",
    "    sigma2_eps = synth_gauss['sigma2_eps']\n",
    "    sigma_eps = np.sqrt(sigma2_eps)\n",
    "    eps = np.random.normal(0, sigma_eps, n)                       # Parameterzied in terms of sigma (not sigma2)\n",
    "\n",
    "    ### data generation ###\n",
    "    X = np.random.multivariate_normal(mu, Sigma2, n)              # Generate & standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    score = X @ beta1 + eps                                       # Generate labels\n",
    "    prob = 1/(1+np.exp(-score))\n",
    "    Y_disc = (prob > 0.5) * 1\n",
    "    \n",
    "    return X, Y_disc, n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for dimension: 50\n",
      "Number of training samples: 2500\n",
      "training set accuracy: 0.9092\n",
      "test set accuracy: 0.9056\n",
      "length delta 0.09664016587839812\n",
      "(50,)\n",
      "0.007172721044610176\n",
      "0.4999999999999999\n"
     ]
    }
   ],
   "source": [
    "epsd = synth_gauss['epsd']\n",
    "for i in synth_gauss['d']:\n",
    "    \n",
    "    ######################\n",
    "    ### GENERATE DATA ###\n",
    "    #####################\n",
    "    print(f'Computing for dimension: {i}')\n",
    "    \n",
    "    X, Y_disc, n_train = gen_lin_data(d=i, synth_gauss=synth_gauss)\n",
    "    \n",
    "    # split data\n",
    "    X, X_prime, Y, Y_prime = train_test_split(X.astype(float), \n",
    "                                              Y_disc.astype(float), \n",
    "                                              test_size=0.5,\n",
    "                                              random_state=10)\n",
    "\n",
    "    Y_train = Y[0:n_train]\n",
    "    Y_test = Y[n_train::]\n",
    "    X_train = X[0:n_train, :]\n",
    "    X_test = X[n_train::, :]\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    ######################\n",
    "    ###    FIT MODEL   ###\n",
    "    ######################\n",
    "    \n",
    "    if synth_gauss['fit_sgd']:\n",
    "        clf = SGDClassifier(loss='log', penalty='none', fit_intercept=True, max_iter=1500, tol=1e-6)\n",
    "        clf.fit(X_train, Y_train)\n",
    "    else:\n",
    "        clf = LogisticRegression(penalty='none', fit_intercept=True, max_iter=1500)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "    print('training set accuracy:', clf.score(X_train, Y_train))\n",
    "    print('test set accuracy:', clf.score(X_test, Y_test))\n",
    "    \n",
    "    f_train = np.log(clf.predict_proba(X_train)[:,1] + epsd) - np.log(clf.predict_proba(X_train)[:,0] + epsd)\n",
    "    w_train = clf.coef_[0]\n",
    "    \n",
    "    \n",
    "    k = 3\n",
    "    t = 0\n",
    "    \n",
    "    cov = (X_train.T @ X_train) / (X_train.shape[0] - 1)\n",
    "    eig_values, eig_vectors = np.linalg.eig(cov)\n",
    "    idx = np.argsort(eig_values, axis=0)[::-1]\n",
    "    sorted_eig_vectors = eig_vectors[:, idx]\n",
    "    W = sorted_eig_vectors[:, :k]\n",
    "    Z = np.dot(X, W)\n",
    "    Xhat = np.dot(Z, W.T)\n",
    "                      \n",
    "    x = X_train[0,:]\n",
    "    z = Z[0,:]\n",
    "    xhat = Xhat[0,:]\n",
    "    rx = x - W @ z\n",
    "    ry = synth_gauss['s'] - f_train[0]\n",
    "    rxy = ry + w_train @ rx\n",
    "    w_tilde = W.T @ w_train\n",
    "\n",
    "    delta_z = (ry / np.linalg.norm(w_tilde, 2)**2) * w_tilde\n",
    "    print('length delta', np.linalg.norm(delta,2))\n",
    "    cf_z = z + delta_z\n",
    "    cf = x + W @ delta_z\n",
    "    print(cf.shape)\n",
    "    cf = cf.reshape(1,-1)\n",
    "    print(clf.predict_proba(x.reshape(1,-1))[0][1])\n",
    "    print(clf.predict_proba(cf)[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_robustnes",
   "language": "python",
   "name": "env_robustnes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
