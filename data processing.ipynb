{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'churn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outs(df, std_treshold: float = 3.5):    #\n",
    "    df = df[np.abs(df-df.mean()) <= (std_treshold*df.std())]\n",
    "    # keep only the ones that are within +std_treshold to -std_treshold standard deviations in the column 'Data'.\n",
    "    df = df[~(np.abs(df-df.mean()) > (std_treshold*df.std()))]\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def remove_colls(df, corr_threshold: float = 0.8):\n",
    "    # Drop multicollinear features\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find features with correlation greater than corr_threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "    # Drop features \n",
    "    df = df.drop(to_drop, axis=1).astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataname == 'communities':\n",
    "    df = pd.read_csv(f'data/{dataname}.csv', header=None)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.drop(columns=[0, 1, 2, 3, 4, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126])\n",
    "    df = df.dropna().astype(float)\n",
    "    Y = df[127] \n",
    "    Y = (Y > np.median(Y)) * 1\n",
    "    df = df.drop(columns=[127])\n",
    "    df = remove_colls(df, corr_threshold=0.9)\n",
    "\n",
    "elif dataname == 'housing':\n",
    "    df = pd.read_csv(f'data/{dataname}.csv')\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    Y = df['median_house_value'] \n",
    "    Y = (Y > np.median(Y))\n",
    "    df = df.drop(columns=['median_house_value'])\n",
    "    df['ocean_proximity'] = df['ocean_proximity'].replace(['NEAR BAY', 'ISLAND', 'NEAR OCEAN', '<1H OCEAN', 'INLAND'], [0, 0, 0, 1, 2])\n",
    "    df = df.astype(float)\n",
    "    df = remove_colls(df, corr_threshold=0.9)\n",
    "\n",
    "elif dataname == \"heloc\":\n",
    "    df = pd.read_csv(f'data/{dataname}.csv')\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns=['RiskPerformance'])\n",
    "    df = df.dropna()\n",
    "    Y = df['ExternalRiskEstimate'] \n",
    "    Y = (Y >= np.median(Y)) * 1\n",
    "    df = df.drop(columns=['ExternalRiskEstimate'])\n",
    "    df = df.astype(float)\n",
    "    df = remove_colls(df, corr_threshold=0.9)\n",
    "    \n",
    "elif dataname == 'speeddating':\n",
    "    df = pd.read_csv(f'data/{dataname}.csv', header=None)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    names = np.arange(df.columns.shape[0])\n",
    "    names = list(map(str, names))\n",
    "    df.columns = names\n",
    "    Y = df['120'] # whether someone matched\n",
    "    df = df.drop(columns=['2', '7', '8', '120'])\n",
    "    df['6'] = df['6'].map({'[0-1]': 1, '[2-3]': 2, '[4-5]': 3, '[4-6]': 4, '[7-37]': 5})\n",
    "    df['12'] = df['12'].map({'[0-1]': 1, '[2-5]': 2, '[6-10]': 3})\n",
    "    df['13'] = df['13'].map({'[0-1]': 1, '[2-5]': 2, '[6-10]': 3})\n",
    "    \n",
    "elif dataname == 'default':\n",
    "    df = pd.read_csv(f'data/{dataname}.csv', header=None)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    ind_nondefault = np.where(df.values[:,-1] == 0)[0]\n",
    "    n_default = ind_nondefault.shape[0]\n",
    "    ind_default = np.where(df.values[:,-1] > 0)[0]\n",
    "    ind_nondefault_subset = np.random.choice(ind_nondefault, ind_default.shape[0], replace=False)\n",
    "    inds = np.r_[ind_default, ind_nondefault_subset]\n",
    "    df = df.iloc[inds]\n",
    "    Y = (df.values[:,-1] >= np.median(df.values[:,-1])) * 1\n",
    "    df = df.drop(columns=df.columns[-1])\n",
    "    df = df.astype(float)\n",
    "    df = remove_colls(df, corr_threshold=0.95)\n",
    "\n",
    "elif dataname == 'mnist':\n",
    "    df_train = pd.read_csv(f'data/{dataname}_train.csv')\n",
    "    df_test = pd.read_csv(f'data/{dataname}_test.csv')\n",
    "    df = pd.concat([ df_train,  df_test], ignore_index=True)\n",
    "    y0_ind = np.where(df['label']==3)[0]\n",
    "    y1_ind = np.where(df['label']==8)[0]\n",
    "    inds = np.r_[y0_ind , y1_ind]\n",
    "    df = df.iloc[inds]\n",
    "    df['label'] = df['label'].map({3:0, 8:1})\n",
    "    Y = (df['label'].values) * 1\n",
    "    df = df.drop(columns=['label'])\n",
    "    #df = df.iloc[np.random.choice(df.shape[0], int(0.30*df.shape[0]), replace=False)]\n",
    "    #df['label'] = df['label'].map({0:0, 1:0, 2:0, 4:0, 7:0,  5:1, 3:1, 6:1, 8:1, 9:1})\n",
    "    \n",
    "elif dataname == 'fraud':\n",
    "    df = pd.read_csv(f'data/{dataname}.csv')\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    '''\n",
    "    ind_fraud = np.where(df['fraud_bool'] == 1)[0]\n",
    "    n_fraud = ind_fraud.shape[0]\n",
    "    ind_nonfraud = np.where(df['fraud_bool'] == 0)[0]\n",
    "    ind_nonfraud_subset = np.random.choice(ind_nonfraud, n_fraud, replace=False)\n",
    "    inds = np.r_[ind_fraud, ind_nonfraud_subset]\n",
    "    df = df.iloc[inds]\n",
    "    Y = df['fraud_bool']\n",
    "    df = df.drop(columns=['fraud_bool'])\n",
    "    '''\n",
    "    for i in [\"payment_type\", \"employment_status\", \"housing_status\", \"source\", \"device_os\"]:\n",
    "        dummies = pd.get_dummies(df[i])\n",
    "        df[dummies.columns] = dummies\n",
    "    df = df.drop(columns=[\"payment_type\", \"employment_status\", \"housing_status\", \"source\", \"device_os\"])\n",
    "    df = df.iloc[np.random.choice(df.shape[0], int(0.05*df.shape[0]), replace=False)]\n",
    "    Y = df['income'] \n",
    "    Y = (Y >= np.median(Y)) * 1\n",
    "    df = df.drop(columns=['income'])\n",
    "elif dataname == 'churn':\n",
    "    df = pd.read_csv(f'data/{dataname}.csv', header=None)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.replace('?', np.nan)\n",
    "    df[230] = df[230].map({-1:0, 1:1})\n",
    "    ind_nondefault = np.where(df.values[:,-1] == 0)[0]\n",
    "    n_default = ind_nondefault.shape[0]\n",
    "    ind_default = np.where(df.values[:,-1] > 0)[0]\n",
    "    ind_nondefault_subset = np.random.choice(ind_nondefault, ind_default.shape[0], replace=False)\n",
    "    inds = np.r_[ind_default, ind_nondefault_subset]\n",
    "    df = df.iloc[inds]\n",
    "    Y = df[230]\n",
    "    df = df.drop(columns=[230])\n",
    "    df = df.iloc[:,0:190]         # keep numerical features only\n",
    "    df = df.fillna(df.median())   # fill missing with median\n",
    "    cols = df.columns[df.isna().any()].tolist()\n",
    "    df = df.drop(columns=cols)\n",
    "    df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7344, 174)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.5\n"
     ]
    }
   ],
   "source": [
    "baseline = np.mean(Y)\n",
    "print('Baseline score:', baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = df.values\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, \n",
    "                                                    Y.astype(float), \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.6399782135076253\n",
      "test set accuracy: 0.5955882352941176\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l2', C=100, fit_intercept=True, max_iter=2000)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print('training set accuracy:', clf.score(X_train, Y_train))\n",
    "print('test set accuracy:', clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement over baseline: 0.09558823529411764\n"
     ]
    }
   ],
   "source": [
    "print('Improvement over baseline:', clf.score(X_test, Y_test) - baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.8619281045751634\n",
      "test set accuracy: 0.6244553376906318\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=10)\n",
    "clf.fit(X_train, Y_train)\n",
    "print('training set accuracy:', clf.score(X_train, Y_train))\n",
    "print('test set accuracy:', clf.score(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
